08/24/2022 21:54:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ../pretrained_model/vocab.txt
08/24/2022 21:54:46 - INFO - __main__ -   ***** Running training *****
08/24/2022 21:54:46 - INFO - __main__ -     Num orig examples = 816
08/24/2022 21:54:46 - INFO - __main__ -     Num split examples = 815
08/24/2022 21:54:46 - INFO - __main__ -     Batch size = 4
08/24/2022 21:54:46 - INFO - __main__ -     Num steps = 306
08/24/2022 21:54:46 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ../pretrained_model
08/24/2022 21:54:46 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

08/24/2022 21:54:48 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
c:\Users\manling2\Anaconda3\lib\site-packages\pytorch_pretrained_bert\optimization.py:275: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  C:\cb\pytorch_1000000000000\work\torch\csrc\utils\python_arg_parser.cpp:1055.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
08/24/2022 21:56:11 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ../pretrained_model/vocab.txt
08/24/2022 21:56:13 - INFO - __main__ -   ***** Running evaluation *****
08/24/2022 21:56:13 - INFO - __main__ -     Num examples = 351
08/24/2022 21:56:13 - INFO - __main__ -     Batch size = 8
